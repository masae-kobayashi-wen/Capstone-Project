{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Dating Data\n",
    "https://www.kaggle.com/annavictoria/speed-dating-experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature Engineering/Pre-processing & Training Data Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from pandas_profiling import ProfileReport\n",
    "from scipy import stats\n",
    "\n",
    "from sb_utils import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if I need this\n",
    "import datetime\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data wrangling notebook, we have identified our target dependent variable as desicion of parner of the specific subject, dec_o (might also consider match, desicion from both the subject and the partner) and cleaned the data accordingly. In this notebook, we will conduct further EDA, hoping to answer following questions.\n",
    "\n",
    "1. The difference of desirable attributes in a male partner vs female partner.\n",
    "2. The difference of desirable attributes among  races.\n",
    "3. The difference of desirable major of male partner vs female partner\n",
    "4. The difference of desirable majors among races.  \n",
    "\n",
    "**Learning Objectives**:\n",
    "1. Understand the importance of creating a model training development data set.\n",
    "2. Correctly identify when to create dummy features or one-hot encoded features.\n",
    "3. Understand the importance of magnitude standardization.\n",
    "4. Apply the train and test split to the development dataset effectively\n",
    "\n",
    "Since speed dating data is relatively clean we may not need to perform 2&3 pre-processing\n",
    "\n",
    "Here is possible workflow: TBD\n",
    "- Use stats.model package for logistic regression model (sloves classification problem): for my model notebook\n",
    "    - import statsmodels.api as sm (This model  is kind of doing the similar thing as ANOVA)\n",
    "- Apply this on the whole data set including the dec_o\n",
    "- Use ‘Speed_Dating_data_cleaned.csv’ from data wrangling output\n",
    "- Fill the missing data (NaN) with mean to model\n",
    "- Use PCA to choose features (but will loose interpretability)\n",
    "- Keep components 0-5 for ~90% var. \n",
    "- Use stepwise selection, elastic-net (or L1/L2 regularizers) \n",
    "    - Statsmodels should have the code to run this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/ski_data_cleaned.csv')\n",
    "spd = pd.read_csv('spd_data_wrangling_output/Speed_Dating_data_cleaned.csv') #spd1_2 in data wrangling notebook\n",
    "spd_fp = pd.read_csv('spd_data_wrangling_output/Speed_Dating_data_FemaleRatingMale_cleaned.csv') # spd1_2fp in data wrangling notebook \n",
    "spd_mp = pd.read_csv('spd_data_wrangling_output/Speed_Dating_data_MaleRatingFemale_cleaned.csv') # spd1_2mp in data wrangling notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6266 entries, 0 to 6265\n",
      "Data columns (total 24 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   gender    6266 non-null   int64  \n",
      " 1   match     6266 non-null   int64  \n",
      " 2   age       6198 non-null   float64\n",
      " 3   race      6208 non-null   float64\n",
      " 4   field     6208 non-null   object \n",
      " 5   career    6182 non-null   object \n",
      " 6   from      6192 non-null   object \n",
      " 7   goal      6192 non-null   float64\n",
      " 8   int_corr  6118 non-null   float64\n",
      " 9   samerace  6266 non-null   int64  \n",
      " 10  imprace   6192 non-null   float64\n",
      " 11  imprelig  6192 non-null   float64\n",
      " 12  age_o     6189 non-null   float64\n",
      " 13  race_o    6198 non-null   float64\n",
      " 14  dec_o     6266 non-null   int64  \n",
      " 15  attr_o    6127 non-null   float64\n",
      " 16  sinc_o    6064 non-null   float64\n",
      " 17  intel_o   6054 non-null   float64\n",
      " 18  fun_o     5999 non-null   float64\n",
      " 19  amb_o     5709 non-null   float64\n",
      " 20  shar_o    5399 non-null   float64\n",
      " 21  like_o    6104 non-null   float64\n",
      " 22  prob_o    6054 non-null   float64\n",
      " 23  met_o     5997 non-null   float64\n",
      "dtypes: float64(17), int64(4), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "spd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>match</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>field</th>\n",
       "      <th>career</th>\n",
       "      <th>from</th>\n",
       "      <th>goal</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>...</th>\n",
       "      <th>dec_o</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Law</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Law</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Law</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Law</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Law</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  match   age  race field  career     from  goal  int_corr  samerace  \\\n",
       "0       0      0  21.0   4.0   Law  lawyer  Chicago   2.0      0.14         0   \n",
       "1       0      0  21.0   4.0   Law  lawyer  Chicago   2.0      0.54         0   \n",
       "2       0      1  21.0   4.0   Law  lawyer  Chicago   2.0      0.16         1   \n",
       "3       0      1  21.0   4.0   Law  lawyer  Chicago   2.0      0.61         0   \n",
       "4       0      1  21.0   4.0   Law  lawyer  Chicago   2.0      0.21         0   \n",
       "\n",
       "   ...  dec_o  attr_o  sinc_o  intel_o  fun_o  amb_o  shar_o  like_o  prob_o  \\\n",
       "0  ...      0     6.0     8.0      8.0    8.0    8.0     6.0     7.0     4.0   \n",
       "1  ...      0     7.0     8.0     10.0    7.0    7.0     5.0     8.0     4.0   \n",
       "2  ...      1    10.0    10.0     10.0   10.0   10.0    10.0    10.0    10.0   \n",
       "3  ...      1     7.0     8.0      9.0    8.0    9.0     8.0     7.0     7.0   \n",
       "4  ...      1     8.0     7.0      9.0    6.0    9.0     7.0     8.0     6.0   \n",
       "\n",
       "   met_o  \n",
       "0    2.0  \n",
       "1    2.0  \n",
       "2    1.0  \n",
       "3    2.0  \n",
       "4    2.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 filling NaN with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new df\n",
    "spd_mean = spd.fillna(spd.mean())\n",
    "spd_fp_mean = spd_fp.fillna(spd_fp.mean())\n",
    "spd_mp_mean = spd_mp.fillna(spd_mp.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender       0\n",
       "match        0\n",
       "age          0\n",
       "race         0\n",
       "field       58\n",
       "career      84\n",
       "from        74\n",
       "goal         0\n",
       "int_corr     0\n",
       "samerace     0\n",
       "imprace      0\n",
       "imprelig     0\n",
       "age_o        0\n",
       "race_o       0\n",
       "dec_o        0\n",
       "attr_o       0\n",
       "sinc_o       0\n",
       "intel_o      0\n",
       "fun_o        0\n",
       "amb_o        0\n",
       "shar_o       0\n",
       "like_o       0\n",
       "prob_o       0\n",
       "met_o        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NaN\n",
    "spd_mean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6266, 24)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender       0\n",
       "match        0\n",
       "age          0\n",
       "race         0\n",
       "field       20\n",
       "career      20\n",
       "from        20\n",
       "goal         0\n",
       "int_corr     0\n",
       "samerace     0\n",
       "imprace      0\n",
       "imprelig     0\n",
       "age_o        0\n",
       "race_o       0\n",
       "dec_o        0\n",
       "attr_o       0\n",
       "sinc_o       0\n",
       "intel_o      0\n",
       "fun_o        0\n",
       "amb_o        0\n",
       "shar_o       0\n",
       "like_o       0\n",
       "prob_o       0\n",
       "met_o        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_fp_mean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3138, 24)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_fp_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender       0\n",
       "match        0\n",
       "age          0\n",
       "race         0\n",
       "field       38\n",
       "career      64\n",
       "from        54\n",
       "goal         0\n",
       "int_corr     0\n",
       "samerace     0\n",
       "imprace      0\n",
       "imprelig     0\n",
       "age_o        0\n",
       "race_o       0\n",
       "dec_o        0\n",
       "attr_o       0\n",
       "sinc_o       0\n",
       "intel_o      0\n",
       "fun_o        0\n",
       "amb_o        0\n",
       "shar_o       0\n",
       "like_o       0\n",
       "prob_o       0\n",
       "met_o        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mp_mean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3128, 24)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mp_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 drop columns with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new df\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "spd_mean1 = spd_mean.dropna(axis='columns')\n",
    "spd_fp_mean1 = spd_fp_mean.dropna(axis='columns')\n",
    "spd_mp_mean1 = spd_fp_mean.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender      0\n",
       "match       0\n",
       "age         0\n",
       "race        0\n",
       "goal        0\n",
       "int_corr    0\n",
       "samerace    0\n",
       "imprace     0\n",
       "imprelig    0\n",
       "age_o       0\n",
       "race_o      0\n",
       "dec_o       0\n",
       "attr_o      0\n",
       "sinc_o      0\n",
       "intel_o     0\n",
       "fun_o       0\n",
       "amb_o       0\n",
       "shar_o      0\n",
       "like_o      0\n",
       "prob_o      0\n",
       "met_o       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NaN\n",
    "spd_mean1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6266, 21)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mean1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender      0\n",
       "match       0\n",
       "age         0\n",
       "race        0\n",
       "goal        0\n",
       "int_corr    0\n",
       "samerace    0\n",
       "imprace     0\n",
       "imprelig    0\n",
       "age_o       0\n",
       "race_o      0\n",
       "dec_o       0\n",
       "attr_o      0\n",
       "sinc_o      0\n",
       "intel_o     0\n",
       "fun_o       0\n",
       "amb_o       0\n",
       "shar_o      0\n",
       "like_o      0\n",
       "prob_o      0\n",
       "met_o       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_fp_mean1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3138, 21)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_fp_mean1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender      0\n",
       "match       0\n",
       "age         0\n",
       "race        0\n",
       "goal        0\n",
       "int_corr    0\n",
       "samerace    0\n",
       "imprace     0\n",
       "imprelig    0\n",
       "age_o       0\n",
       "race_o      0\n",
       "dec_o       0\n",
       "attr_o      0\n",
       "sinc_o      0\n",
       "intel_o     0\n",
       "fun_o       0\n",
       "amb_o       0\n",
       "shar_o      0\n",
       "like_o      0\n",
       "prob_o      0\n",
       "met_o       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mp_mean1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3138, 21)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mp_mean1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>match</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>goal</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>age_o</th>\n",
       "      <th>...</th>\n",
       "      <th>dec_o</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3138.0</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "      <td>3138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.170491</td>\n",
       "      <td>26.434041</td>\n",
       "      <td>2.672867</td>\n",
       "      <td>2.225786</td>\n",
       "      <td>0.192213</td>\n",
       "      <td>0.404398</td>\n",
       "      <td>3.459589</td>\n",
       "      <td>3.116421</td>\n",
       "      <td>26.079288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360421</td>\n",
       "      <td>5.976672</td>\n",
       "      <td>7.210102</td>\n",
       "      <td>7.535608</td>\n",
       "      <td>6.345409</td>\n",
       "      <td>7.056627</td>\n",
       "      <td>5.505990</td>\n",
       "      <td>6.062459</td>\n",
       "      <td>5.241818</td>\n",
       "      <td>1.956957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376123</td>\n",
       "      <td>3.387402</td>\n",
       "      <td>1.199342</td>\n",
       "      <td>1.487494</td>\n",
       "      <td>0.300429</td>\n",
       "      <td>0.490853</td>\n",
       "      <td>2.655817</td>\n",
       "      <td>2.578369</td>\n",
       "      <td>3.608140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480199</td>\n",
       "      <td>1.963168</td>\n",
       "      <td>1.786063</td>\n",
       "      <td>1.553134</td>\n",
       "      <td>1.998559</td>\n",
       "      <td>1.714952</td>\n",
       "      <td>2.033149</td>\n",
       "      <td>1.898075</td>\n",
       "      <td>2.116420</td>\n",
       "      <td>0.287586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.830000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.192213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.345409</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.505990</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gender        match          age         race         goal  \\\n",
       "count  3138.0  3138.000000  3138.000000  3138.000000  3138.000000   \n",
       "mean      1.0     0.170491    26.434041     2.672867     2.225786   \n",
       "std       0.0     0.376123     3.387402     1.199342     1.487494   \n",
       "min       1.0     0.000000    18.000000     1.000000     1.000000   \n",
       "25%       1.0     0.000000    24.000000     2.000000     1.000000   \n",
       "50%       1.0     0.000000    27.000000     2.000000     2.000000   \n",
       "75%       1.0     0.000000    28.000000     4.000000     3.000000   \n",
       "max       1.0     1.000000    42.000000     6.000000     6.000000   \n",
       "\n",
       "          int_corr     samerace      imprace     imprelig        age_o  ...  \\\n",
       "count  3138.000000  3138.000000  3138.000000  3138.000000  3138.000000  ...   \n",
       "mean      0.192213     0.404398     3.459589     3.116421    26.079288  ...   \n",
       "std       0.300429     0.490853     2.655817     2.578369     3.608140  ...   \n",
       "min      -0.830000     0.000000     1.000000     1.000000    19.000000  ...   \n",
       "25%      -0.010000     0.000000     1.000000     1.000000    23.000000  ...   \n",
       "50%       0.192213     0.000000     3.000000     2.000000    26.000000  ...   \n",
       "75%       0.420000     1.000000     6.000000     5.000000    28.000000  ...   \n",
       "max       0.900000     1.000000    10.000000    10.000000    38.000000  ...   \n",
       "\n",
       "             dec_o       attr_o       sinc_o      intel_o        fun_o  \\\n",
       "count  3138.000000  3138.000000  3138.000000  3138.000000  3138.000000   \n",
       "mean      0.360421     5.976672     7.210102     7.535608     6.345409   \n",
       "std       0.480199     1.963168     1.786063     1.553134     1.998559   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     5.000000     6.000000     7.000000     5.000000   \n",
       "50%       0.000000     6.000000     7.000000     8.000000     6.345409   \n",
       "75%       1.000000     7.000000     8.000000     9.000000     8.000000   \n",
       "max       1.000000    10.500000    10.000000    10.000000    10.000000   \n",
       "\n",
       "             amb_o       shar_o       like_o       prob_o        met_o  \n",
       "count  3138.000000  3138.000000  3138.000000  3138.000000  3138.000000  \n",
       "mean      7.056627     5.505990     6.062459     5.241818     1.956957  \n",
       "std       1.714952     2.033149     1.898075     2.116420     0.287586  \n",
       "min       0.000000     0.000000     0.000000     0.000000     1.000000  \n",
       "25%       6.000000     5.000000     5.000000     4.000000     2.000000  \n",
       "50%       7.000000     5.505990     6.000000     5.000000     2.000000  \n",
       "75%       8.000000     7.000000     7.000000     7.000000     2.000000  \n",
       "max      10.000000    10.000000    10.000000    10.000000     8.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mp_mean1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Extracting more seemingly relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to 'Speed dating_2_EDA_mk'\n",
    "spd_mean1_mini = spd_mean1.loc[:, 'dec_o':'prob_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dec_o      0\n",
       "attr_o     0\n",
       "sinc_o     0\n",
       "intel_o    0\n",
       "fun_o      0\n",
       "amb_o      0\n",
       "shar_o     0\n",
       "like_o     0\n",
       "prob_o     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "spd_mean1_mini.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6266, 9)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mean1_mini.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dec_o</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.427705</td>\n",
       "      <td>6.233132</td>\n",
       "      <td>7.223615</td>\n",
       "      <td>7.403204</td>\n",
       "      <td>6.418736</td>\n",
       "      <td>6.826152</td>\n",
       "      <td>5.554269</td>\n",
       "      <td>6.166317</td>\n",
       "      <td>5.233812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.494785</td>\n",
       "      <td>1.912573</td>\n",
       "      <td>1.689306</td>\n",
       "      <td>1.502261</td>\n",
       "      <td>1.910492</td>\n",
       "      <td>1.689681</td>\n",
       "      <td>1.987716</td>\n",
       "      <td>1.826321</td>\n",
       "      <td>2.106286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.403204</td>\n",
       "      <td>6.418736</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.554269</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dec_o       attr_o       sinc_o      intel_o        fun_o  \\\n",
       "count  6266.000000  6266.000000  6266.000000  6266.000000  6266.000000   \n",
       "mean      0.427705     6.233132     7.223615     7.403204     6.418736   \n",
       "std       0.494785     1.912573     1.689306     1.502261     1.910492   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     5.000000     6.000000     7.000000     5.000000   \n",
       "50%       0.000000     6.000000     7.000000     7.403204     6.418736   \n",
       "75%       1.000000     8.000000     8.000000     8.000000     8.000000   \n",
       "max       1.000000    10.500000    10.000000    10.000000    11.000000   \n",
       "\n",
       "             amb_o       shar_o       like_o       prob_o  \n",
       "count  6266.000000  6266.000000  6266.000000  6266.000000  \n",
       "mean      6.826152     5.554269     6.166317     5.233812  \n",
       "std       1.689681     1.987716     1.826321     2.106286  \n",
       "min       0.000000     0.000000     0.000000     0.000000  \n",
       "25%       6.000000     5.000000     5.000000     4.000000  \n",
       "50%       7.000000     5.554269     6.000000     5.000000  \n",
       "75%       8.000000     7.000000     7.000000     7.000000  \n",
       "max      10.000000    10.000000    10.000000    10.000000  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mean1_mini.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Scale the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>match</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>goal</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>age_o</th>\n",
       "      <th>...</th>\n",
       "      <th>dec_o</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.001597</td>\n",
       "      <td>-0.453793</td>\n",
       "      <td>-1.499401</td>\n",
       "      <td>1.078539</td>\n",
       "      <td>-0.107357</td>\n",
       "      <td>-0.175110</td>\n",
       "      <td>-0.825102</td>\n",
       "      <td>-0.629879</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>0.209980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.864495</td>\n",
       "      <td>-0.121904</td>\n",
       "      <td>0.459625</td>\n",
       "      <td>0.397297</td>\n",
       "      <td>0.827739</td>\n",
       "      <td>0.694772</td>\n",
       "      <td>0.224261</td>\n",
       "      <td>0.456518</td>\n",
       "      <td>-0.585823</td>\n",
       "      <td>0.169905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.001597</td>\n",
       "      <td>-0.453793</td>\n",
       "      <td>-1.499401</td>\n",
       "      <td>1.078539</td>\n",
       "      <td>-0.107357</td>\n",
       "      <td>1.156634</td>\n",
       "      <td>-0.825102</td>\n",
       "      <td>-0.629879</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>-1.218746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.864495</td>\n",
       "      <td>0.400994</td>\n",
       "      <td>0.459625</td>\n",
       "      <td>1.728729</td>\n",
       "      <td>0.304272</td>\n",
       "      <td>0.102896</td>\n",
       "      <td>-0.278870</td>\n",
       "      <td>1.004111</td>\n",
       "      <td>-0.585823</td>\n",
       "      <td>0.169905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.001597</td>\n",
       "      <td>2.203650</td>\n",
       "      <td>-1.499401</td>\n",
       "      <td>1.078539</td>\n",
       "      <td>-0.107357</td>\n",
       "      <td>-0.108523</td>\n",
       "      <td>1.211971</td>\n",
       "      <td>-0.629879</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>-1.218746</td>\n",
       "      <td>...</td>\n",
       "      <td>1.156745</td>\n",
       "      <td>1.969687</td>\n",
       "      <td>1.643637</td>\n",
       "      <td>1.728729</td>\n",
       "      <td>1.874673</td>\n",
       "      <td>1.878522</td>\n",
       "      <td>2.236781</td>\n",
       "      <td>2.099296</td>\n",
       "      <td>2.263021</td>\n",
       "      <td>-3.704318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.001597</td>\n",
       "      <td>2.203650</td>\n",
       "      <td>-1.499401</td>\n",
       "      <td>1.078539</td>\n",
       "      <td>-0.107357</td>\n",
       "      <td>1.389689</td>\n",
       "      <td>-0.825102</td>\n",
       "      <td>-0.629879</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>-0.933000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.156745</td>\n",
       "      <td>0.400994</td>\n",
       "      <td>0.459625</td>\n",
       "      <td>1.063013</td>\n",
       "      <td>0.827739</td>\n",
       "      <td>1.286647</td>\n",
       "      <td>1.230521</td>\n",
       "      <td>0.456518</td>\n",
       "      <td>0.838599</td>\n",
       "      <td>0.169905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.001597</td>\n",
       "      <td>2.203650</td>\n",
       "      <td>-1.499401</td>\n",
       "      <td>1.078539</td>\n",
       "      <td>-0.107357</td>\n",
       "      <td>0.057945</td>\n",
       "      <td>-0.825102</td>\n",
       "      <td>-0.629879</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>-0.647255</td>\n",
       "      <td>...</td>\n",
       "      <td>1.156745</td>\n",
       "      <td>0.923891</td>\n",
       "      <td>-0.132381</td>\n",
       "      <td>1.063013</td>\n",
       "      <td>-0.219195</td>\n",
       "      <td>1.286647</td>\n",
       "      <td>0.727391</td>\n",
       "      <td>1.004111</td>\n",
       "      <td>0.363792</td>\n",
       "      <td>0.169905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender     match       age      race      goal  int_corr  samerace  \\\n",
       "0 -1.001597 -0.453793 -1.499401  1.078539 -0.107357 -0.175110 -0.825102   \n",
       "1 -1.001597 -0.453793 -1.499401  1.078539 -0.107357  1.156634 -0.825102   \n",
       "2 -1.001597  2.203650 -1.499401  1.078539 -0.107357 -0.108523  1.211971   \n",
       "3 -1.001597  2.203650 -1.499401  1.078539 -0.107357  1.389689 -0.825102   \n",
       "4 -1.001597  2.203650 -1.499401  1.078539 -0.107357  0.057945 -0.825102   \n",
       "\n",
       "    imprace  imprelig     age_o  ...     dec_o    attr_o    sinc_o   intel_o  \\\n",
       "0 -0.629879  0.094728  0.209980  ... -0.864495 -0.121904  0.459625  0.397297   \n",
       "1 -0.629879  0.094728 -1.218746  ... -0.864495  0.400994  0.459625  1.728729   \n",
       "2 -0.629879  0.094728 -1.218746  ...  1.156745  1.969687  1.643637  1.728729   \n",
       "3 -0.629879  0.094728 -0.933000  ...  1.156745  0.400994  0.459625  1.063013   \n",
       "4 -0.629879  0.094728 -0.647255  ...  1.156745  0.923891 -0.132381  1.063013   \n",
       "\n",
       "      fun_o     amb_o    shar_o    like_o    prob_o     met_o  \n",
       "0  0.827739  0.694772  0.224261  0.456518 -0.585823  0.169905  \n",
       "1  0.304272  0.102896 -0.278870  1.004111 -0.585823  0.169905  \n",
       "2  1.874673  1.878522  2.236781  2.099296  2.263021 -3.704318  \n",
       "3  0.827739  1.286647  1.230521  0.456518  0.838599  0.169905  \n",
       "4 -0.219195  1.286647  0.727391  1.004111  0.363792  0.169905  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refer to '6_GuidedCapstone/04_preprocessing_and_training_mk'\n",
    "# refer to '16.3.1_Capstone_Two_Step_4__Preprocessing_Training_Data_Development.pdf'\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Making a Scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting data (spd_mean1, df w/o NaN) to the scaler object\n",
    "spd_mean1_scaled = scaler.fit_transform(spd_mean1)\n",
    "print(type(spd_mean1_scaled))\n",
    "spd_mean1_scaled = pd.DataFrame(spd_mean1_scaled, columns=spd_mean1.columns)\n",
    "spd_mean1_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6266, 21)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mean1_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>match</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>goal</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>age_o</th>\n",
       "      <th>...</th>\n",
       "      <th>dec_o</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.382808e-16</td>\n",
       "      <td>9.466308e-16</td>\n",
       "      <td>-7.147437e-16</td>\n",
       "      <td>2.325338e-16</td>\n",
       "      <td>-9.428524e-16</td>\n",
       "      <td>7.907637e-17</td>\n",
       "      <td>-6.520656e-16</td>\n",
       "      <td>2.865035e-17</td>\n",
       "      <td>-1.834366e-16</td>\n",
       "      <td>6.846139e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.994377e-17</td>\n",
       "      <td>-6.506127e-16</td>\n",
       "      <td>-1.096438e-15</td>\n",
       "      <td>-4.078732e-16</td>\n",
       "      <td>-5.045437e-16</td>\n",
       "      <td>2.618751e-16</td>\n",
       "      <td>5.847718e-16</td>\n",
       "      <td>6.311581e-16</td>\n",
       "      <td>4.669636e-15</td>\n",
       "      <td>-3.420173e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.001597e+00</td>\n",
       "      <td>-4.537925e-01</td>\n",
       "      <td>-2.355032e+00</td>\n",
       "      <td>-1.393671e+00</td>\n",
       "      <td>-8.108032e-01</td>\n",
       "      <td>-3.404588e+00</td>\n",
       "      <td>-8.251024e-01</td>\n",
       "      <td>-9.788916e-01</td>\n",
       "      <td>-9.775884e-01</td>\n",
       "      <td>-2.361726e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.644945e-01</td>\n",
       "      <td>-3.259291e+00</td>\n",
       "      <td>-4.276425e+00</td>\n",
       "      <td>-4.928435e+00</td>\n",
       "      <td>-3.359997e+00</td>\n",
       "      <td>-4.040229e+00</td>\n",
       "      <td>-2.794521e+00</td>\n",
       "      <td>-3.376629e+00</td>\n",
       "      <td>-2.485052e+00</td>\n",
       "      <td>-3.704318e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.001597e+00</td>\n",
       "      <td>-4.537925e-01</td>\n",
       "      <td>-6.437707e-01</td>\n",
       "      <td>-5.696011e-01</td>\n",
       "      <td>-8.108032e-01</td>\n",
       "      <td>-6.745135e-01</td>\n",
       "      <td>-8.251024e-01</td>\n",
       "      <td>-9.788916e-01</td>\n",
       "      <td>-9.775884e-01</td>\n",
       "      <td>-6.472551e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.644945e-01</td>\n",
       "      <td>-6.448019e-01</td>\n",
       "      <td>-7.243876e-01</td>\n",
       "      <td>-2.684198e-01</td>\n",
       "      <td>-7.426617e-01</td>\n",
       "      <td>-4.889786e-01</td>\n",
       "      <td>-2.788696e-01</td>\n",
       "      <td>-6.386665e-01</td>\n",
       "      <td>-5.858231e-01</td>\n",
       "      <td>1.699051e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.984054e-01</td>\n",
       "      <td>-4.537925e-01</td>\n",
       "      <td>-7.335029e-02</td>\n",
       "      <td>-5.696011e-01</td>\n",
       "      <td>-1.073573e-01</td>\n",
       "      <td>2.465184e-02</td>\n",
       "      <td>-8.251024e-01</td>\n",
       "      <td>-2.808670e-01</td>\n",
       "      <td>-2.627107e-01</td>\n",
       "      <td>-7.576472e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.644945e-01</td>\n",
       "      <td>-1.219042e-01</td>\n",
       "      <td>-1.323813e-01</td>\n",
       "      <td>-5.912749e-16</td>\n",
       "      <td>-4.649321e-16</td>\n",
       "      <td>1.028965e-01</td>\n",
       "      <td>4.468694e-16</td>\n",
       "      <td>-9.107402e-02</td>\n",
       "      <td>-1.110158e-01</td>\n",
       "      <td>1.699051e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.984054e-01</td>\n",
       "      <td>-4.537925e-01</td>\n",
       "      <td>4.970702e-01</td>\n",
       "      <td>1.078539e+00</td>\n",
       "      <td>-1.073573e-01</td>\n",
       "      <td>7.571108e-01</td>\n",
       "      <td>1.211971e+00</td>\n",
       "      <td>7.661700e-01</td>\n",
       "      <td>8.096060e-01</td>\n",
       "      <td>4.957257e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.156745e+00</td>\n",
       "      <td>9.238912e-01</td>\n",
       "      <td>4.596249e-01</td>\n",
       "      <td>3.972966e-01</td>\n",
       "      <td>8.277393e-01</td>\n",
       "      <td>6.947715e-01</td>\n",
       "      <td>7.273908e-01</td>\n",
       "      <td>4.565184e-01</td>\n",
       "      <td>8.385988e-01</td>\n",
       "      <td>1.699051e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.984054e-01</td>\n",
       "      <td>2.203650e+00</td>\n",
       "      <td>4.490013e+00</td>\n",
       "      <td>2.726678e+00</td>\n",
       "      <td>2.706426e+00</td>\n",
       "      <td>2.355203e+00</td>\n",
       "      <td>1.211971e+00</td>\n",
       "      <td>2.162219e+00</td>\n",
       "      <td>2.239362e+00</td>\n",
       "      <td>4.496158e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.156745e+00</td>\n",
       "      <td>2.231136e+00</td>\n",
       "      <td>1.643637e+00</td>\n",
       "      <td>1.728729e+00</td>\n",
       "      <td>2.398140e+00</td>\n",
       "      <td>1.878522e+00</td>\n",
       "      <td>2.236781e+00</td>\n",
       "      <td>2.099296e+00</td>\n",
       "      <td>2.263021e+00</td>\n",
       "      <td>2.341524e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             gender         match           age          race          goal  \\\n",
       "count  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03   \n",
       "mean   6.382808e-16  9.466308e-16 -7.147437e-16  2.325338e-16 -9.428524e-16   \n",
       "std    1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00   \n",
       "min   -1.001597e+00 -4.537925e-01 -2.355032e+00 -1.393671e+00 -8.108032e-01   \n",
       "25%   -1.001597e+00 -4.537925e-01 -6.437707e-01 -5.696011e-01 -8.108032e-01   \n",
       "50%    9.984054e-01 -4.537925e-01 -7.335029e-02 -5.696011e-01 -1.073573e-01   \n",
       "75%    9.984054e-01 -4.537925e-01  4.970702e-01  1.078539e+00 -1.073573e-01   \n",
       "max    9.984054e-01  2.203650e+00  4.490013e+00  2.726678e+00  2.706426e+00   \n",
       "\n",
       "           int_corr      samerace       imprace      imprelig         age_o  \\\n",
       "count  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03   \n",
       "mean   7.907637e-17 -6.520656e-16  2.865035e-17 -1.834366e-16  6.846139e-16   \n",
       "std    1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00   \n",
       "min   -3.404588e+00 -8.251024e-01 -9.788916e-01 -9.775884e-01 -2.361726e+00   \n",
       "25%   -6.745135e-01 -8.251024e-01 -9.788916e-01 -9.775884e-01 -6.472551e-01   \n",
       "50%    2.465184e-02 -8.251024e-01 -2.808670e-01 -2.627107e-01 -7.576472e-02   \n",
       "75%    7.571108e-01  1.211971e+00  7.661700e-01  8.096060e-01  4.957257e-01   \n",
       "max    2.355203e+00  1.211971e+00  2.162219e+00  2.239362e+00  4.496158e+00   \n",
       "\n",
       "       ...         dec_o        attr_o        sinc_o       intel_o  \\\n",
       "count  ...  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03   \n",
       "mean   ... -2.994377e-17 -6.506127e-16 -1.096438e-15 -4.078732e-16   \n",
       "std    ...  1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00   \n",
       "min    ... -8.644945e-01 -3.259291e+00 -4.276425e+00 -4.928435e+00   \n",
       "25%    ... -8.644945e-01 -6.448019e-01 -7.243876e-01 -2.684198e-01   \n",
       "50%    ... -8.644945e-01 -1.219042e-01 -1.323813e-01 -5.912749e-16   \n",
       "75%    ...  1.156745e+00  9.238912e-01  4.596249e-01  3.972966e-01   \n",
       "max    ...  1.156745e+00  2.231136e+00  1.643637e+00  1.728729e+00   \n",
       "\n",
       "              fun_o         amb_o        shar_o        like_o        prob_o  \\\n",
       "count  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03   \n",
       "mean  -5.045437e-16  2.618751e-16  5.847718e-16  6.311581e-16  4.669636e-15   \n",
       "std    1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00   \n",
       "min   -3.359997e+00 -4.040229e+00 -2.794521e+00 -3.376629e+00 -2.485052e+00   \n",
       "25%   -7.426617e-01 -4.889786e-01 -2.788696e-01 -6.386665e-01 -5.858231e-01   \n",
       "50%   -4.649321e-16  1.028965e-01  4.468694e-16 -9.107402e-02 -1.110158e-01   \n",
       "75%    8.277393e-01  6.947715e-01  7.273908e-01  4.565184e-01  8.385988e-01   \n",
       "max    2.398140e+00  1.878522e+00  2.236781e+00  2.099296e+00  2.263021e+00   \n",
       "\n",
       "              met_o  \n",
       "count  6.266000e+03  \n",
       "mean  -3.420173e-15  \n",
       "std    1.000080e+00  \n",
       "min   -3.704318e+00  \n",
       "25%    1.699051e-01  \n",
       "50%    1.699051e-01  \n",
       "75%    1.699051e-01  \n",
       "max    2.341524e+01  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_mean1_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 Set up input data for logistic Regression Model (X and y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spd_mean1\n",
    "X = spd_mean1.drop(columns='dec_o')\n",
    "y = spd_mean1['dec_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6266, 20), (6266,))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>match</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>goal</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>age_o</th>\n",
       "      <th>race_o</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500798</td>\n",
       "      <td>0.170763</td>\n",
       "      <td>26.257180</td>\n",
       "      <td>2.691205</td>\n",
       "      <td>2.152616</td>\n",
       "      <td>0.192596</td>\n",
       "      <td>0.405043</td>\n",
       "      <td>3.804748</td>\n",
       "      <td>3.734981</td>\n",
       "      <td>26.265148</td>\n",
       "      <td>2.690384</td>\n",
       "      <td>6.233132</td>\n",
       "      <td>7.223615</td>\n",
       "      <td>7.403204</td>\n",
       "      <td>6.418736</td>\n",
       "      <td>6.826152</td>\n",
       "      <td>5.554269</td>\n",
       "      <td>6.166317</td>\n",
       "      <td>5.233812</td>\n",
       "      <td>1.956145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500039</td>\n",
       "      <td>0.376332</td>\n",
       "      <td>3.506465</td>\n",
       "      <td>1.213586</td>\n",
       "      <td>1.421687</td>\n",
       "      <td>0.300382</td>\n",
       "      <td>0.490940</td>\n",
       "      <td>2.865457</td>\n",
       "      <td>2.797904</td>\n",
       "      <td>3.499901</td>\n",
       "      <td>1.212258</td>\n",
       "      <td>1.912573</td>\n",
       "      <td>1.689306</td>\n",
       "      <td>1.502261</td>\n",
       "      <td>1.910492</td>\n",
       "      <td>1.689681</td>\n",
       "      <td>1.987716</td>\n",
       "      <td>1.826321</td>\n",
       "      <td>2.106286</td>\n",
       "      <td>0.258137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.830000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.403204</td>\n",
       "      <td>6.418736</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.554269</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender        match          age         race         goal  \\\n",
       "count  6266.000000  6266.000000  6266.000000  6266.000000  6266.000000   \n",
       "mean      0.500798     0.170763    26.257180     2.691205     2.152616   \n",
       "std       0.500039     0.376332     3.506465     1.213586     1.421687   \n",
       "min       0.000000     0.000000    18.000000     1.000000     1.000000   \n",
       "25%       0.000000     0.000000    24.000000     2.000000     1.000000   \n",
       "50%       1.000000     0.000000    26.000000     2.000000     2.000000   \n",
       "75%       1.000000     0.000000    28.000000     4.000000     2.000000   \n",
       "max       1.000000     1.000000    42.000000     6.000000     6.000000   \n",
       "\n",
       "          int_corr     samerace      imprace     imprelig        age_o  \\\n",
       "count  6266.000000  6266.000000  6266.000000  6266.000000  6266.000000   \n",
       "mean      0.192596     0.405043     3.804748     3.734981    26.265148   \n",
       "std       0.300382     0.490940     2.865457     2.797904     3.499901   \n",
       "min      -0.830000     0.000000     1.000000     1.000000    18.000000   \n",
       "25%      -0.010000     0.000000     1.000000     1.000000    24.000000   \n",
       "50%       0.200000     0.000000     3.000000     3.000000    26.000000   \n",
       "75%       0.420000     1.000000     6.000000     6.000000    28.000000   \n",
       "max       0.900000     1.000000    10.000000    10.000000    42.000000   \n",
       "\n",
       "            race_o       attr_o       sinc_o      intel_o        fun_o  \\\n",
       "count  6266.000000  6266.000000  6266.000000  6266.000000  6266.000000   \n",
       "mean      2.690384     6.233132     7.223615     7.403204     6.418736   \n",
       "std       1.212258     1.912573     1.689306     1.502261     1.910492   \n",
       "min       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       2.000000     5.000000     6.000000     7.000000     5.000000   \n",
       "50%       2.000000     6.000000     7.000000     7.403204     6.418736   \n",
       "75%       4.000000     8.000000     8.000000     8.000000     8.000000   \n",
       "max       6.000000    10.500000    10.000000    10.000000    11.000000   \n",
       "\n",
       "             amb_o       shar_o       like_o       prob_o        met_o  \n",
       "count  6266.000000  6266.000000  6266.000000  6266.000000  6266.000000  \n",
       "mean      6.826152     5.554269     6.166317     5.233812     1.956145  \n",
       "std       1.689681     1.987716     1.826321     2.106286     0.258137  \n",
       "min       0.000000     0.000000     0.000000     0.000000     1.000000  \n",
       "25%       6.000000     5.000000     5.000000     4.000000     2.000000  \n",
       "50%       7.000000     5.554269     6.000000     5.000000     2.000000  \n",
       "75%       8.000000     7.000000     7.000000     7.000000     2.000000  \n",
       "max      10.000000    10.000000    10.000000    10.000000     8.000000  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need scaling to use X (whole) as input. Will try both methods: 1) extract relavant features from X (scale 10), 2) use whole feature X with scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spd_mean1_mini\n",
    "Xm = spd_mean1_mini.drop(columns='dec_o')\n",
    "ym = spd_mean1_mini['dec_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6266, 8), (6266,))"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "Xm.shape, ym.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "      <td>6266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.233132</td>\n",
       "      <td>7.223615</td>\n",
       "      <td>7.403204</td>\n",
       "      <td>6.418736</td>\n",
       "      <td>6.826152</td>\n",
       "      <td>5.554269</td>\n",
       "      <td>6.166317</td>\n",
       "      <td>5.233812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.912573</td>\n",
       "      <td>1.689306</td>\n",
       "      <td>1.502261</td>\n",
       "      <td>1.910492</td>\n",
       "      <td>1.689681</td>\n",
       "      <td>1.987716</td>\n",
       "      <td>1.826321</td>\n",
       "      <td>2.106286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.403204</td>\n",
       "      <td>6.418736</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.554269</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            attr_o       sinc_o      intel_o        fun_o        amb_o  \\\n",
       "count  6266.000000  6266.000000  6266.000000  6266.000000  6266.000000   \n",
       "mean      6.233132     7.223615     7.403204     6.418736     6.826152   \n",
       "std       1.912573     1.689306     1.502261     1.910492     1.689681   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       5.000000     6.000000     7.000000     5.000000     6.000000   \n",
       "50%       6.000000     7.000000     7.403204     6.418736     7.000000   \n",
       "75%       8.000000     8.000000     8.000000     8.000000     8.000000   \n",
       "max      10.500000    10.000000    10.000000    11.000000    10.000000   \n",
       "\n",
       "            shar_o       like_o       prob_o  \n",
       "count  6266.000000  6266.000000  6266.000000  \n",
       "mean      5.554269     6.166317     5.233812  \n",
       "std       1.987716     1.826321     2.106286  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       5.000000     5.000000     4.000000  \n",
       "50%       5.554269     6.000000     5.000000  \n",
       "75%       7.000000     7.000000     7.000000  \n",
       "max      10.000000    10.000000    10.000000  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spd_mean1_scaled\n",
    "Xs = spd_mean1_scaled.drop(columns='dec_o')\n",
    "ys = spd_mean1_scaled['dec_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6266, 20), (6266,))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "Xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>match</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>goal</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>age_o</th>\n",
       "      <th>race_o</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "      <td>6.266000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.382808e-16</td>\n",
       "      <td>9.466308e-16</td>\n",
       "      <td>-7.147437e-16</td>\n",
       "      <td>2.325338e-16</td>\n",
       "      <td>-9.428524e-16</td>\n",
       "      <td>7.907637e-17</td>\n",
       "      <td>-6.520656e-16</td>\n",
       "      <td>2.865035e-17</td>\n",
       "      <td>-1.834366e-16</td>\n",
       "      <td>6.846139e-16</td>\n",
       "      <td>1.418414e-15</td>\n",
       "      <td>-6.506127e-16</td>\n",
       "      <td>-1.096438e-15</td>\n",
       "      <td>-4.078732e-16</td>\n",
       "      <td>-5.045437e-16</td>\n",
       "      <td>2.618751e-16</td>\n",
       "      <td>5.847718e-16</td>\n",
       "      <td>6.311581e-16</td>\n",
       "      <td>4.669636e-15</td>\n",
       "      <td>-3.420173e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "      <td>1.000080e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.001597e+00</td>\n",
       "      <td>-4.537925e-01</td>\n",
       "      <td>-2.355032e+00</td>\n",
       "      <td>-1.393671e+00</td>\n",
       "      <td>-8.108032e-01</td>\n",
       "      <td>-3.404588e+00</td>\n",
       "      <td>-8.251024e-01</td>\n",
       "      <td>-9.788916e-01</td>\n",
       "      <td>-9.775884e-01</td>\n",
       "      <td>-2.361726e+00</td>\n",
       "      <td>-1.394521e+00</td>\n",
       "      <td>-3.259291e+00</td>\n",
       "      <td>-4.276425e+00</td>\n",
       "      <td>-4.928435e+00</td>\n",
       "      <td>-3.359997e+00</td>\n",
       "      <td>-4.040229e+00</td>\n",
       "      <td>-2.794521e+00</td>\n",
       "      <td>-3.376629e+00</td>\n",
       "      <td>-2.485052e+00</td>\n",
       "      <td>-3.704318e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.001597e+00</td>\n",
       "      <td>-4.537925e-01</td>\n",
       "      <td>-6.437707e-01</td>\n",
       "      <td>-5.696011e-01</td>\n",
       "      <td>-8.108032e-01</td>\n",
       "      <td>-6.745135e-01</td>\n",
       "      <td>-8.251024e-01</td>\n",
       "      <td>-9.788916e-01</td>\n",
       "      <td>-9.775884e-01</td>\n",
       "      <td>-6.472551e-01</td>\n",
       "      <td>-5.695482e-01</td>\n",
       "      <td>-6.448019e-01</td>\n",
       "      <td>-7.243876e-01</td>\n",
       "      <td>-2.684198e-01</td>\n",
       "      <td>-7.426617e-01</td>\n",
       "      <td>-4.889786e-01</td>\n",
       "      <td>-2.788696e-01</td>\n",
       "      <td>-6.386665e-01</td>\n",
       "      <td>-5.858231e-01</td>\n",
       "      <td>1.699051e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.984054e-01</td>\n",
       "      <td>-4.537925e-01</td>\n",
       "      <td>-7.335029e-02</td>\n",
       "      <td>-5.696011e-01</td>\n",
       "      <td>-1.073573e-01</td>\n",
       "      <td>2.465184e-02</td>\n",
       "      <td>-8.251024e-01</td>\n",
       "      <td>-2.808670e-01</td>\n",
       "      <td>-2.627107e-01</td>\n",
       "      <td>-7.576472e-02</td>\n",
       "      <td>-5.695482e-01</td>\n",
       "      <td>-1.219042e-01</td>\n",
       "      <td>-1.323813e-01</td>\n",
       "      <td>-5.912749e-16</td>\n",
       "      <td>-4.649321e-16</td>\n",
       "      <td>1.028965e-01</td>\n",
       "      <td>4.468694e-16</td>\n",
       "      <td>-9.107402e-02</td>\n",
       "      <td>-1.110158e-01</td>\n",
       "      <td>1.699051e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.984054e-01</td>\n",
       "      <td>-4.537925e-01</td>\n",
       "      <td>4.970702e-01</td>\n",
       "      <td>1.078539e+00</td>\n",
       "      <td>-1.073573e-01</td>\n",
       "      <td>7.571108e-01</td>\n",
       "      <td>1.211971e+00</td>\n",
       "      <td>7.661700e-01</td>\n",
       "      <td>8.096060e-01</td>\n",
       "      <td>4.957257e-01</td>\n",
       "      <td>1.080398e+00</td>\n",
       "      <td>9.238912e-01</td>\n",
       "      <td>4.596249e-01</td>\n",
       "      <td>3.972966e-01</td>\n",
       "      <td>8.277393e-01</td>\n",
       "      <td>6.947715e-01</td>\n",
       "      <td>7.273908e-01</td>\n",
       "      <td>4.565184e-01</td>\n",
       "      <td>8.385988e-01</td>\n",
       "      <td>1.699051e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.984054e-01</td>\n",
       "      <td>2.203650e+00</td>\n",
       "      <td>4.490013e+00</td>\n",
       "      <td>2.726678e+00</td>\n",
       "      <td>2.706426e+00</td>\n",
       "      <td>2.355203e+00</td>\n",
       "      <td>1.211971e+00</td>\n",
       "      <td>2.162219e+00</td>\n",
       "      <td>2.239362e+00</td>\n",
       "      <td>4.496158e+00</td>\n",
       "      <td>2.730344e+00</td>\n",
       "      <td>2.231136e+00</td>\n",
       "      <td>1.643637e+00</td>\n",
       "      <td>1.728729e+00</td>\n",
       "      <td>2.398140e+00</td>\n",
       "      <td>1.878522e+00</td>\n",
       "      <td>2.236781e+00</td>\n",
       "      <td>2.099296e+00</td>\n",
       "      <td>2.263021e+00</td>\n",
       "      <td>2.341524e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gender         match           age          race          goal  \\\n",
       "count  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03   \n",
       "mean   6.382808e-16  9.466308e-16 -7.147437e-16  2.325338e-16 -9.428524e-16   \n",
       "std    1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00   \n",
       "min   -1.001597e+00 -4.537925e-01 -2.355032e+00 -1.393671e+00 -8.108032e-01   \n",
       "25%   -1.001597e+00 -4.537925e-01 -6.437707e-01 -5.696011e-01 -8.108032e-01   \n",
       "50%    9.984054e-01 -4.537925e-01 -7.335029e-02 -5.696011e-01 -1.073573e-01   \n",
       "75%    9.984054e-01 -4.537925e-01  4.970702e-01  1.078539e+00 -1.073573e-01   \n",
       "max    9.984054e-01  2.203650e+00  4.490013e+00  2.726678e+00  2.706426e+00   \n",
       "\n",
       "           int_corr      samerace       imprace      imprelig         age_o  \\\n",
       "count  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03   \n",
       "mean   7.907637e-17 -6.520656e-16  2.865035e-17 -1.834366e-16  6.846139e-16   \n",
       "std    1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00   \n",
       "min   -3.404588e+00 -8.251024e-01 -9.788916e-01 -9.775884e-01 -2.361726e+00   \n",
       "25%   -6.745135e-01 -8.251024e-01 -9.788916e-01 -9.775884e-01 -6.472551e-01   \n",
       "50%    2.465184e-02 -8.251024e-01 -2.808670e-01 -2.627107e-01 -7.576472e-02   \n",
       "75%    7.571108e-01  1.211971e+00  7.661700e-01  8.096060e-01  4.957257e-01   \n",
       "max    2.355203e+00  1.211971e+00  2.162219e+00  2.239362e+00  4.496158e+00   \n",
       "\n",
       "             race_o        attr_o        sinc_o       intel_o         fun_o  \\\n",
       "count  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03   \n",
       "mean   1.418414e-15 -6.506127e-16 -1.096438e-15 -4.078732e-16 -5.045437e-16   \n",
       "std    1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00   \n",
       "min   -1.394521e+00 -3.259291e+00 -4.276425e+00 -4.928435e+00 -3.359997e+00   \n",
       "25%   -5.695482e-01 -6.448019e-01 -7.243876e-01 -2.684198e-01 -7.426617e-01   \n",
       "50%   -5.695482e-01 -1.219042e-01 -1.323813e-01 -5.912749e-16 -4.649321e-16   \n",
       "75%    1.080398e+00  9.238912e-01  4.596249e-01  3.972966e-01  8.277393e-01   \n",
       "max    2.730344e+00  2.231136e+00  1.643637e+00  1.728729e+00  2.398140e+00   \n",
       "\n",
       "              amb_o        shar_o        like_o        prob_o         met_o  \n",
       "count  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03  6.266000e+03  \n",
       "mean   2.618751e-16  5.847718e-16  6.311581e-16  4.669636e-15 -3.420173e-15  \n",
       "std    1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00  1.000080e+00  \n",
       "min   -4.040229e+00 -2.794521e+00 -3.376629e+00 -2.485052e+00 -3.704318e+00  \n",
       "25%   -4.889786e-01 -2.788696e-01 -6.386665e-01 -5.858231e-01  1.699051e-01  \n",
       "50%    1.028965e-01  4.468694e-16 -9.107402e-02 -1.110158e-01  1.699051e-01  \n",
       "75%    6.947715e-01  7.273908e-01  4.565184e-01  8.385988e-01  1.699051e-01  \n",
       "max    1.878522e+00  2.236781e+00  2.099296e+00  2.263021e+00  2.341524e+01  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -0.864495\n",
       "1      -0.864495\n",
       "2       1.156745\n",
       "3       1.156745\n",
       "4       1.156745\n",
       "          ...   \n",
       "6261   -0.864495\n",
       "6262   -0.864495\n",
       "6263   -0.864495\n",
       "6264   -0.864495\n",
       "6265   -0.864495\n",
       "Name: dec_o, Length: 6266, dtype: float64"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaled depedent variable (dec_o) for logistic regression fitting does not work!\n",
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Training Data Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 LogisticRegression via sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1.1 Use X, y as whole without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 100000000.0}\n",
      "Best score is 0.8282119708738058\n",
      "Accuracy on training data: 0.83\n",
      "Accuracy on test data:     0.82\n",
      "[[647  67]\n",
      " [163 377]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.91      0.85       714\n",
      "           1       0.85      0.70      0.77       540\n",
      "\n",
      "    accuracy                           0.82      1254\n",
      "   macro avg       0.82      0.80      0.81      1254\n",
      "weighted avg       0.82      0.82      0.81      1254\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# refer to '14.1.2_3_Supervised Learning_FineTuning'\n",
    "# Import necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "# apply best estimators to test set\n",
    "# refer to '14.2.11 Logistic Regression Advanced Case Study_mk'\n",
    "logreg_best = logreg_cv.best_estimator_\n",
    "training_accuracy = logreg_best.score(X_train, y_train)\n",
    "test_accuracy = logreg_best.score(X_test, y_test)\n",
    "print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n",
    "\n",
    "# other way to check the data: confusion\n",
    "# refer to '14.1.2_3_SupervisedLearning_Tuning'\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = logreg_best.predict(X_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to scale the dataset if using X, y as a whole (from spd_mean1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1.2 Use Xm, ym: extracted relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.4393970560760795}\n",
      "Best score is 0.7711496249773633\n",
      "Accuracy on training data: 0.77\n",
      "Accuracy on test data:     0.76\n",
      "[[587 127]\n",
      " [170 370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       714\n",
      "           1       0.74      0.69      0.71       540\n",
      "\n",
      "    accuracy                           0.76      1254\n",
      "   macro avg       0.76      0.75      0.76      1254\n",
      "weighted avg       0.76      0.76      0.76      1254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic regresion w/o penalty\n",
    "# refer to '14.1.2_3_Supervised Learning_Tuning'\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(Xm, ym, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(Xm_train, ym_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "# apply best estimators to test set\n",
    "# refer to '14.2.11 Logistic Regression Advanced Case Study_mk'\n",
    "logreg_best = logreg_cv.best_estimator_\n",
    "training_accuracy = logreg_best.score(Xm_train, ym_train)\n",
    "test_accuracy = logreg_best.score(Xm_test, ym_test)\n",
    "print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n",
    "\n",
    "# other way to check the data: confusion\n",
    "# refer to '14.1.2_3_SupervisedLearning_Tuning'\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "ym_pred = logreg_best.predict(Xm_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(ym_test, ym_pred))\n",
    "print(classification_report(ym_test, ym_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\mkoba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.4393970560760795, 'penalty': 'l2'}\n",
      "Best score is 0.7711496249773633\n",
      "Accuracy on training data: 0.77\n",
      "Accuracy on test data:     0.76\n",
      "[[587 127]\n",
      " [170 370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       714\n",
      "           1       0.74      0.69      0.71       540\n",
      "\n",
      "    accuracy                           0.76      1254\n",
      "   macro avg       0.76      0.75      0.76      1254\n",
      "weighted avg       0.76      0.76      0.76      1254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic regresion w penalty\n",
    "# refer to '14.1.2_3_Supervised Learning_Tuning'\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(Xm, ym, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']} \n",
    "#%%%%% this time 'l1' does not work with the code, when I can check for both 'l1' and 'l2' when I can't?\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(Xm_train, ym_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "# apply best estimators to test set\n",
    "# refer to '14.2.11 Logistic Regression Advanced Case Study_mk'\n",
    "logreg_best = logreg_cv.best_estimator_\n",
    "training_accuracy = logreg_best.score(Xm_train, ym_train)\n",
    "test_accuracy = logreg_best.score(Xm_test, ym_test)\n",
    "print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n",
    "\n",
    "# other way to check the data: confusion\n",
    "# refer to '14.1.2_3_SupervisedLearning_Tuning'\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "ym_pred = logreg_best.predict(Xm_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(ym_test, ym_pred))\n",
    "print(classification_report(ym_test, ym_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.4393970560760795, 'penalty': 'l2'}\n",
      "Best score is 0.7711496249773633\n",
      "Accuracy on training data: 0.77\n",
      "Accuracy on test data:     0.76\n",
      "[[587 127]\n",
      " [170 370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       714\n",
      "           1       0.74      0.69      0.71       540\n",
      "\n",
      "    accuracy                           0.76      1254\n",
      "   macro avg       0.76      0.75      0.76      1254\n",
      "weighted avg       0.76      0.76      0.76      1254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic regresion w penalty\n",
    "# refer to '14.1.2_3_Supervised Learning_Tuning'\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(Xm, ym, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l2']} # 'l2' penalty works b/c default default ‘lbfgs’ solvers support only l2 penalties.\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(Xm_train, ym_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "# apply best estimators to test set\n",
    "# refer to '14.2.11 Logistic Regression Advanced Case Study_mk'\n",
    "logreg_best = logreg_cv.best_estimator_\n",
    "training_accuracy = logreg_best.score(Xm_train, ym_train)\n",
    "test_accuracy = logreg_best.score(Xm_test, ym_test)\n",
    "print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n",
    "\n",
    "# other way to check the data: confusion\n",
    "# refer to '14.1.2_3_SupervisedLearning_Tuning'\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "ym_pred = logreg_best.predict(Xm_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(ym_test, ym_pred))\n",
    "print(classification_report(ym_test, ym_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'l2' (Ridge) penalty term did not improve the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.4393970560760795, 'l1_ratio': 0.4, 'penalty': 'elasticnet'}\n",
      "Best score is 0.7715486275703827\n",
      "Accuracy on training data: 0.77\n",
      "Accuracy on test data:     0.76\n",
      "[[587 127]\n",
      " [170 370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       714\n",
      "           1       0.74      0.69      0.71       540\n",
      "\n",
      "    accuracy                           0.76      1254\n",
      "   macro avg       0.76      0.75      0.76      1254\n",
      "weighted avg       0.76      0.76      0.76      1254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# refer to 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html'\n",
    "\n",
    "# logistic regresion w elastic penalty\n",
    "# refer to '14.1.2_3_Supervised Learning_Tuning'\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(Xm, ym, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "l1_space = np.arange(0,1,0.1)\n",
    "param_grid = {'C': c_space, 'penalty': ['elasticnet'], 'l1_ratio': l1_space} \n",
    "\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression(solver='saga', max_iter=10000) \n",
    "#%%%%%% need to specify solver and max_inter (default setting does not work)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(Xm_train, ym_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "# apply best estimators to test set\n",
    "# refer to '14.2.11 Logistic Regression Advanced Case Study_mk'\n",
    "logreg_best = logreg_cv.best_estimator_\n",
    "training_accuracy = logreg_best.score(Xm_train, ym_train)\n",
    "test_accuracy = logreg_best.score(Xm_test, ym_test)\n",
    "print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n",
    "\n",
    "# other way to check the data: confusion\n",
    "# refer to '14.1.2_3_SupervisedLearning_Tuning'\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "ym_pred = logreg_best.predict(Xm_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(ym_test, ym_pred))\n",
    "print(classification_report(ym_test, ym_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elasticnet penalty improved the model a little (slightly higher tp and preciscion for 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.05179474679231213, 'l1_ratio': 0.4, 'penalty': 'elasticnet'}\n",
      "Best score is 0.8525581234505676\n",
      "Accuracy on training data: 0.77\n",
      "Accuracy on test data:     0.76\n",
      "[[586 128]\n",
      " [172 368]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80       714\n",
      "           1       0.74      0.68      0.71       540\n",
      "\n",
      "    accuracy                           0.76      1254\n",
      "   macro avg       0.76      0.75      0.75      1254\n",
      "weighted avg       0.76      0.76      0.76      1254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use scoring = 'roc_auc' instead of 'accuracy' \n",
    "#%%%%%% should not use 'accuracy' scoring for data with imbalanced disribution.\n",
    "# for logistic regression, target variable (0,1), if one has much more than the other (more than 10x > imbalanced)\n",
    "\n",
    "# refer to 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html'\n",
    "\n",
    "# logistic regresion w elastic penalty\n",
    "# refer to '14.1.2_3_Supervised Learning_Tuning'\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(Xm, ym, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "l1_space = np.arange(0,1,0.1)\n",
    "param_grid = {'C': c_space, 'penalty': ['elasticnet'], 'l1_ratio': l1_space} \n",
    "\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression(solver='saga', max_iter=10000) \n",
    "#%%%%%% need to specify solver and max_inter (default setting does not work)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, scoring='roc_auc', cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(Xm_train, ym_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "# apply best estimators to test set\n",
    "# refer to '14.2.11 Logistic Regression Advanced Case Study_mk'\n",
    "logreg_best = logreg_cv.best_estimator_\n",
    "training_accuracy = logreg_best.score(Xm_train, ym_train)\n",
    "test_accuracy = logreg_best.score(Xm_test, ym_test)\n",
    "print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n",
    "\n",
    "# other way to check the data: confusion\n",
    "# refer to '14.1.2_3_SupervisedLearning_Tuning'\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "ym_pred = logreg_best.predict(Xm_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(ym_test, ym_pred))\n",
    "print(classification_report(ym_test, ym_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, pandas.core.series.Series)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ym_pred),type(ym_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ym_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "### should build a pipeline!?? fill nan with median..etc\n",
    "# refer to '14.1.2_4_Supervised Learning with scikit-learn_Preprocessing and Pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1.3 Use Xs, ys as whole with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# refer to \\'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\\'\\n\\n# logistic regresion w elastic penalty\\n# refer to \\'14.1.2_3_Supervised Learning_Tuning\\'\\n# Import necessary modules\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import GridSearchCV\\n\\n# Split the data into a training and test set.\\nXs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, test_size=0.2, random_state=42)\\n\\n# Setup the hyperparameter grid\\nc_space = np.logspace(-5, 8, 15)\\nl1_space = np.arange(0,1,0.1)\\nparam_grid = {\\'C\\': c_space, \\'penalty\\': [\\'elasticnet\\'], \\'l1_ratio\\': l1_space} \\n\\n\\n# Instantiate a logistic regression classifier: logreg\\nlogreg = LogisticRegression(solver=\\'saga\\', max_iter=10000) \\n#%%%%%% need to specify solver and max_inter (default setting does not work)\\n\\n# Instantiate the GridSearchCV object: logreg_cv\\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\\n\\n# Fit it to the data\\nlogreg_cv.fit(Xs_train, ys_train)\\n\\n# Print the tuned parameters and score\\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \\nprint(\"Best score is {}\".format(logreg_cv.best_score_))\\n\\n# apply best estimators to test set\\n# refer to \\'14.2.11 Logistic Regression Advanced Case Study_mk\\'\\nlogreg_best = logreg_cv.best_estimator_\\ntraining_accuracy = logreg_best.score(Xs_train, ys_train)\\ntest_accuracy = logreg_best.score(Xs_test, ys_test)\\nprint(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\\nprint(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\\n\\n# other way to check the data: confusion\\n# refer to \\'14.1.2_3_SupervisedLearning_Tuning\\'\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.metrics import confusion_matrix\\n\\n# Predict the labels of the test data: y_pred\\nys_pred = logreg_best.predict(Xs_test)\\n\\n# Compute and print the confusion matrix and classification report\\nprint(confusion_matrix(ys_test, ys_pred))\\nprint(classification_report(ys_test, ys_pred))\\n'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# refer to 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html'\n",
    "\n",
    "# logistic regresion w elastic penalty\n",
    "# refer to '14.1.2_3_Supervised Learning_Tuning'\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "l1_space = np.arange(0,1,0.1)\n",
    "param_grid = {'C': c_space, 'penalty': ['elasticnet'], 'l1_ratio': l1_space} \n",
    "\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression(solver='saga', max_iter=10000) \n",
    "#%%%%%% need to specify solver and max_inter (default setting does not work)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(Xs_train, ys_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "# apply best estimators to test set\n",
    "# refer to '14.2.11 Logistic Regression Advanced Case Study_mk'\n",
    "logreg_best = logreg_cv.best_estimator_\n",
    "training_accuracy = logreg_best.score(Xs_train, ys_train)\n",
    "test_accuracy = logreg_best.score(Xs_test, ys_test)\n",
    "print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n",
    "\n",
    "# other way to check the data: confusion\n",
    "# refer to '14.1.2_3_SupervisedLearning_Tuning'\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "ys_pred = logreg_best.predict(Xs_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(ys_test, ys_pred))\n",
    "print(classification_report(ys_test, ys_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above gives: **ValueError: Unknown label type: 'continuous'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.05179474679231213, 'l1_ratio': 0.1, 'penalty': 'elasticnet'}\n",
      "Best score is 0.8332033838603948\n",
      "Accuracy on training data: 0.83\n",
      "Accuracy on test data:     0.82\n",
      "[[651  63]\n",
      " [163 377]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.91      0.85       714\n",
      "           1       0.86      0.70      0.77       540\n",
      "\n",
      "    accuracy                           0.82      1254\n",
      "   macro avg       0.83      0.80      0.81      1254\n",
      "weighted avg       0.82      0.82      0.82      1254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use non-scaled dependent variable y!\n",
    "\n",
    "# refer to 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html'\n",
    "# logistic regresion w elastic penalty\n",
    "# refer to '14.1.2_3_Supervised Learning_Tuning'\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the data into a training and test set\n",
    "# scaled depedent variable (dec_o) for logistic regression fitting does not work!\n",
    "# gives error if used ys_train b/c ys is continuous and not 0, 1 to fit logreg!!\n",
    "# need to scale without the dec_o.....so keeing the original y\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "l1_space = np.arange(0,1,0.1)\n",
    "param_grid = {'C': c_space, 'penalty': ['elasticnet'], 'l1_ratio': l1_space} \n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression(solver='saga', max_iter=10000) \n",
    "#%%%%%% need to specify solver and max_inter (default setting does not work)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(Xs_train, ys_train) \n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "# apply best estimators to test set\n",
    "# refer to '14.2.11 Logistic Regression Advanced Case Study_mk'\n",
    "logreg_best = logreg_cv.best_estimator_\n",
    "training_accuracy = logreg_best.score(Xs_train, ys_train)\n",
    "test_accuracy = logreg_best.score(Xs_test, ys_test)\n",
    "print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n",
    "\n",
    "# other way to check the data: confusion\n",
    "# refer to '14.1.2_3_SupervisedLearning_Tuning'\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "ys_pred = logreg_best.predict(Xs_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(ys_test, ys_pred))\n",
    "print(classification_report(ys_test, ys_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model so far, yet run time is much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to 'https://stats.stackexchange.com/questions/59392/should-you-ever-standardise-binary-variables'\n",
    "#%%%%% not sure if above scaling is good enough or need to leave out all the binary variable for scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 LogisticRegression via statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2.1 Use Xm, ym: extracted relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.558129\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "# statsmodel w/o constant\n",
    "# Import the statsmodels module\n",
    "# refer to 'https://www.geeksforgeeks.org/logistic-regression-using-statsmodels/'\n",
    "import statsmodels.api as sm\n",
    "\n",
    "log_reg1 = sm.Logit(ym_train, Xm_train).fit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  dec_o   No. Observations:                 5012\n",
      "Model:                          Logit   Df Residuals:                     5004\n",
      "Method:                           MLE   Df Model:                            7\n",
      "Date:                Sat, 13 Feb 2021   Pseudo R-squ.:                  0.1822\n",
      "Time:                        21:24:01   Log-Likelihood:                -2797.3\n",
      "converged:                       True   LL-Null:                       -3420.4\n",
      "Covariance Type:            nonrobust   LLR p-value:                7.472e-265\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "attr_o         0.2678      0.025     10.737      0.000       0.219       0.317\n",
      "sinc_o        -0.2946      0.029    -10.325      0.000      -0.351      -0.239\n",
      "intel_o       -0.3439      0.033    -10.393      0.000      -0.409      -0.279\n",
      "fun_o          0.1033      0.028      3.735      0.000       0.049       0.157\n",
      "amb_o         -0.2845      0.027    -10.503      0.000      -0.338      -0.231\n",
      "shar_o         0.0406      0.023      1.742      0.082      -0.005       0.086\n",
      "like_o         0.5300      0.035     15.105      0.000       0.461       0.599\n",
      "prob_o         0.0937      0.018      5.081      0.000       0.058       0.130\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# printing the summary table \n",
    "print(log_reg1.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.469677\n",
      "         Iterations 7\n"
     ]
    }
   ],
   "source": [
    "# statsmodel w/ constant\n",
    "# Import the statsmodels module\n",
    "# refer to 'https://www.geeksforgeeks.org/logistic-regression-using-statsmodels/'\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Create constants for X, so the model knows its bounds\n",
    "Xm = sm.add_constant(Xm)\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(Xm, ym, test_size=0.2, random_state=42)\n",
    "\n",
    "log_reg2 = sm.Logit(ym_train, Xm_train).fit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  dec_o   No. Observations:                 5012\n",
      "Model:                          Logit   Df Residuals:                     5003\n",
      "Method:                           MLE   Df Model:                            8\n",
      "Date:                Sat, 13 Feb 2021   Pseudo R-squ.:                  0.3118\n",
      "Time:                        21:24:01   Log-Likelihood:                -2354.0\n",
      "converged:                       True   LL-Null:                       -3420.4\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -6.5588      0.260    -25.228      0.000      -7.068      -6.049\n",
      "attr_o         0.4912      0.029     16.774      0.000       0.434       0.549\n",
      "sinc_o        -0.1694      0.033     -5.212      0.000      -0.233      -0.106\n",
      "intel_o       -0.0187      0.039     -0.479      0.632      -0.095       0.058\n",
      "fun_o          0.1077      0.031      3.504      0.000       0.047       0.168\n",
      "amb_o         -0.1857      0.031     -6.002      0.000      -0.246      -0.125\n",
      "shar_o         0.0672      0.026      2.615      0.009       0.017       0.118\n",
      "like_o         0.5543      0.039     14.237      0.000       0.478       0.631\n",
      "prob_o         0.2070      0.021      9.713      0.000       0.165       0.249\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# printing the summary table \n",
    "print(log_reg2.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher the Pseudo R-squ.score, the better the model is. Adding constant helped improved the Pseudo R-squ. a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.4696773953325422\n",
      "            Iterations: 33\n",
      "            Function evaluations: 35\n",
      "            Gradient evaluations: 33\n"
     ]
    }
   ],
   "source": [
    "# statsmodel w/ constant & penalty term\n",
    "# Import the statsmodels module\n",
    "# refer to 'https://www.geeksforgeeks.org/logistic-regression-using-statsmodels/'\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Create constants for X, so the model knows its bounds\n",
    "Xm = sm.add_constant(Xm)\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(Xm, ym, test_size=0.2, random_state=42)\n",
    "\n",
    "# refer to 'https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.fit_regularized.html'\n",
    "log_reg3 = sm.Logit(ym_train, Xm_train).fit_regularized() # default method='l1' (l2 or elasticnet penalty not available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  dec_o   No. Observations:                 5012\n",
      "Model:                          Logit   Df Residuals:                     5003\n",
      "Method:                           MLE   Df Model:                            8\n",
      "Date:                Sat, 13 Feb 2021   Pseudo R-squ.:                  0.3118\n",
      "Time:                        21:24:01   Log-Likelihood:                -2354.0\n",
      "converged:                       True   LL-Null:                       -3420.4\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -6.5589      0.260    -25.228      0.000      -7.068      -6.049\n",
      "attr_o         0.4912      0.029     16.774      0.000       0.434       0.549\n",
      "sinc_o        -0.1694      0.033     -5.212      0.000      -0.233      -0.106\n",
      "intel_o       -0.0187      0.039     -0.479      0.632      -0.095       0.058\n",
      "fun_o          0.1077      0.031      3.504      0.000       0.047       0.168\n",
      "amb_o         -0.1857      0.031     -6.001      0.000      -0.246      -0.125\n",
      "shar_o         0.0672      0.026      2.615      0.009       0.017       0.118\n",
      "like_o         0.5543      0.039     14.237      0.000       0.478       0.631\n",
      "prob_o         0.2070      0.021      9.712      0.000       0.165       0.249\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# printing the summary table \n",
    "print(log_reg3.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test data: y_pred\n",
    "ym_pred2 = log_reg2.predict(Xm_test)\n",
    "ym_pred3 = log_reg3.predict(Xm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series,\n",
       " pandas.core.series.Series,\n",
       " pandas.core.series.Series)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ym_test), type(ym_pred2), type(ym_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5607    0.441238\n",
       " 2302    0.344809\n",
       " 4672    0.939217\n",
       " 3831    0.146094\n",
       " 6005    0.038742\n",
       "           ...   \n",
       " 4088    0.628473\n",
       " 952     0.137084\n",
       " 3574    0.866432\n",
       " 5880    0.535517\n",
       " 2024    0.093702\n",
       " Length: 1254, dtype: float64,\n",
       " 5607    0.441232\n",
       " 2302    0.344805\n",
       " 4672    0.939217\n",
       " 3831    0.146094\n",
       " 6005    0.038744\n",
       "           ...   \n",
       " 4088    0.628470\n",
       " 952     0.137091\n",
       " 3574    0.866431\n",
       " 5880    0.535515\n",
       " 2024    0.093705\n",
       " Length: 1254, dtype: float64)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ym_pred2, ym_pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      probability\n",
      "5607     0.441238\n",
      "2302     0.344809\n",
      "4672     0.939217\n",
      "3831     0.146094\n",
      "6005     0.038742\n",
      "...           ...\n",
      "4088     0.628473\n",
      "952      0.137084\n",
      "3574     0.866432\n",
      "5880     0.535517\n",
      "2024     0.093702\n",
      "\n",
      "[1254 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5607</th>\n",
       "      <td>0.441238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>0.344809</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>0.939217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>0.146094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>0.038742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4088</th>\n",
       "      <td>0.628473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>0.137084</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>0.866432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>0.535517</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0.093702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1254 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      probability  prediction\n",
       "5607     0.441238           0\n",
       "2302     0.344809           0\n",
       "4672     0.939217           1\n",
       "3831     0.146094           0\n",
       "6005     0.038742           0\n",
       "...           ...         ...\n",
       "4088     0.628473           1\n",
       "952      0.137084           0\n",
       "3574     0.866432           1\n",
       "5880     0.535517           1\n",
       "2024     0.093702           0\n",
       "\n",
       "[1254 rows x 2 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should convert this probability to 0 (p < 0.5), or 1 (p >= 0.5) \n",
    "# in order to compute the confusion matrix and classification report (check the sklearn ym_pred output above)\n",
    "ym_pred2_df = pd.DataFrame({'probability': ym_pred2})\n",
    "print(ym_pred2_df)\n",
    "\n",
    "# hope to use list comprehension\n",
    "# refer to 'https://chrisalbon.com/python/data_wrangling/pandas_list_comprehension/'\n",
    "ym_pred2_df['prediction'] = [0 if row < 0.5 else 1 for row in ym_pred2_df.probability] \n",
    "#%%%% ym_pred_df.prediction = [list comprehenshion] did not work!\n",
    "ym_pred2_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      probability\n",
      "5607     0.441232\n",
      "2302     0.344805\n",
      "4672     0.939217\n",
      "3831     0.146094\n",
      "6005     0.038744\n",
      "...           ...\n",
      "4088     0.628470\n",
      "952      0.137091\n",
      "3574     0.866431\n",
      "5880     0.535515\n",
      "2024     0.093705\n",
      "\n",
      "[1254 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5607</th>\n",
       "      <td>0.441232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>0.344805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>0.939217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>0.146094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>0.038744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4088</th>\n",
       "      <td>0.628470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>0.137091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>0.866431</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>0.535515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0.093705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1254 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      probability  prediction\n",
       "5607     0.441232           0\n",
       "2302     0.344805           0\n",
       "4672     0.939217           1\n",
       "3831     0.146094           0\n",
       "6005     0.038744           0\n",
       "...           ...         ...\n",
       "4088     0.628470           1\n",
       "952      0.137091           0\n",
       "3574     0.866431           1\n",
       "5880     0.535515           1\n",
       "2024     0.093705           0\n",
       "\n",
       "[1254 rows x 2 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ym_pred3_df = pd.DataFrame({'probability': ym_pred3})\n",
    "print(ym_pred3_df)\n",
    "\n",
    "# hope to use list comprehension\n",
    "# refer to 'https://chrisalbon.com/python/data_wrangling/pandas_list_comprehension/'\n",
    "ym_pred3_df['prediction'] = [0 if row < 0.5 else 1 for row in ym_pred3_df.probability] \n",
    "#%%%% ym_pred_df.prediction = [list comprehenshion] did not work!\n",
    "ym_pred3_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[587 127]\n",
      " [170 370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       714\n",
      "           1       0.74      0.69      0.71       540\n",
      "\n",
      "    accuracy                           0.76      1254\n",
      "   macro avg       0.76      0.75      0.76      1254\n",
      "weighted avg       0.76      0.76      0.76      1254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(ym_test, ym_pred2_df.prediction))\n",
    "print(classification_report(ym_test, ym_pred2_df.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[587 127]\n",
      " [170 370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       714\n",
      "           1       0.74      0.69      0.71       540\n",
      "\n",
      "    accuracy                           0.76      1254\n",
      "   macro avg       0.76      0.75      0.76      1254\n",
      "weighted avg       0.76      0.76      0.76      1254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(ym_test, ym_pred3_df.prediction))\n",
    "print(classification_report(ym_test, ym_pred3_df.prediction))\n",
    "# refer to 'https://www.youtube.com/watch?v=Kdsp6soqA7o&ab_channel=StatQuestwithJoshStarmer' to interpret confusion matrix\n",
    "# refer to 'https://www.youtube.com/watch?v=2osIZ-dSPGE&ab_channel=codebasics' to interpret classififation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5607    True\n",
       "2302    True\n",
       "4672    True\n",
       "3831    True\n",
       "6005    True\n",
       "        ... \n",
       "4088    True\n",
       "952     True\n",
       "3574    True\n",
       "5880    True\n",
       "2024    True\n",
       "Name: prediction, Length: 1254, dtype: bool"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ym_pred3_df.prediction == ym_pred2_df.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ym_pred3_df.prediction.all() == ym_pred2_df.prediction.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the penalty term in statsmodel didn't help at all (gave exactly same predition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to '11.4.1_Case Study - Linear Regression/Springboard Regression Case Study - the Red Wine Dataset - Tier 3_mk.ipynb' \n",
    "# for some statsmodel codes to predit from x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to '14.1.2_4_Supervised Learning with scikit-learn_Preprocessing and Pipeline' for some elesticNet code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should try knn, randomforest too!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference\n",
    "- sklearn codes:\n",
    "    - '14.1.2_3_Supervised Learning_Tuning': logisticRegression, confusion_matrix, classification_report, \n",
    "    - '14.1.2_4_Supervised Learning with scikit-learn_Preprocessing and Pipeline': buiding pipeline, scaler, get_dummies() \n",
    "    - '6_GuidedCapstone/04_preprocessing_and_training_mk': scaler, Random Forest model\n",
    "     - '14.2.11_Case Study - Logistic Regression/Logistic Regression Advanced Case Study_mk': plot logisticRegression output.\n",
    "- Statsmodel codes:\n",
    "    - '11.4.1_Case Study - Linear Regression/Regression Case Study - the Red Wine Dataset - Tier 3_mk': sm.OLS(y, X), plot predictions (y_test vs. y_pred)\n",
    "    \n",
    "    \n",
    "- refer to 'https://pandas.pydata.org/pandas-docs/version/0.24.0rc1/api/generated/pandas.Series.to_numpy.html'\n",
    "    - series to numpy: s.to_numpy()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- how do perform cross validation on statsmodel?\n",
    "    - Answers:\n",
    "    - currently not avaliable(can't perform cross validation using statsmodels.api yet) and is not compatibible with sklearn cross_val_score or GredSearchCV...etc\n",
    "    - need to write custom codes if you really want to\n",
    "    - people usually use sklearn for building ML model.\n",
    "    - statsmodel is used for quick stats calculation from a model than model optimazation. \n",
    "- how to use ElasticNet in sklearn LogisticRegression? (currently only l1 or l2 are available!?)\n",
    "    - Answers:\n",
    "    - need to change the default solver (lbfgs) to saga in order to use elasticnet\n",
    "        - param_grid = {'C': c_space, 'penalty': ['elasticnet'], 'l1_ratio': l1_space} \n",
    "        - logreg = LogisticRegression(solver='saga', max_iter=10000) \n",
    "        - logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "- how to compare the performance of statsmodel and sklearn lotistic model\n",
    "    - Answers:\n",
    "    - use sklearn confusion_matrix, classification_report on the y_pred, y_test\n",
    "    - need to convert the statsmodel's y_pred in probability to binary system (0,1) in advance\n",
    "    \n",
    "- how to choose scoring system for GridSearhCV like we can do for cross_val_score? (what is the defaut score?, accuracy?)\n",
    "    - cv_accuracy = cross_val_score(clf, Xlr, ylr, cv=5, scoring='accuracy')\n",
    "    - cv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n",
    "    - Answers:\n",
    "    - use GridSearhCV(logreg, param_grid, scoring = 'roc_auc', cv=5), default scoring = 'accuracy'\n",
    "    - should not use 'accuracy' scoring for data with imbalanced disribution.\n",
    "    - for logistic regression, target variable (0,1), if there is much more (more than 10x!?) 1 than 0 (or vice verso) > imbalanced data\n",
    "- for logistic regression model, do I need to leave all the binary variable (or only dependent variable) out for scaling.\n",
    "    - Answers: \n",
    "    - leaving it out all the binary viarable for scaling for logistic regression model might be better!?\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
